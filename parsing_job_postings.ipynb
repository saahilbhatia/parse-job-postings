{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing raw unstructured text files\n",
    "\n",
    "Environment: Python 3.6.5 and Jupyter notebook\n",
    "\n",
    "Libraries used in the task:\n",
    "* re (for regular expression) \n",
    "* json (for transforming data to json format) \n",
    "\n",
    "## 1. Introduction\n",
    "This notebook extracts data from a huge unstructured text file containing more than 30,000 job postings. The job postings have details about the job like job title, job description, job location, required qualifications, application procedure, application deadline, etc. The extracted data is transformed to XML and JSON formats.  \n",
    "\n",
    "Regular expressions were designed for the section keys of the job postings; the section keys here refer to the key for job ID, job description, title, location, etc. Regular expressions were designed to match all the possible section keys in a job posting. The start and end indices of the section keys in a job posting were retrieved using the re library and then the relevant information for the section was extracted using string slicing method. The extracted data for every job posting was stored in a dictionary with the key as the section key.    \n",
    "\n",
    "The extracted data was transformed to json format using the json library. The extracted data was transformed to XML format using the base python methods: looping, conditional statements, string/list methods. Finally, the transformed data is written to files, which can be used for downstream analysis. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code to import libraries that are needed for this assessment:\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Examining and loading the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('raw_job_postings_data.dat','r') as file:\n",
    "    file_contents = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking** that the data file has been loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72397697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ID: 69290\\nJOB RESPONSIBILITIES:\\n - Develop online/ mobile games working with the team very closely (being\\na team player, not a solo);\\n- Work with Designers and Illustrators on artwork and design\\nimplementation into the games;\\n- Define specifications of game features together with Product Managers;\\n- Develop and architect different types of frameworks and toolsets;\\n- Constantly learn and grow your skills.\\nDEADLINES: 25 January 2014\\nLOCATION: Yerevan, Armenia\\nabout_company:\\n Arplan LLC is an archi'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(file_contents))\n",
    "file_contents[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we examine the data file, we can observe that the job postings are separated by a line: \"-------------\\n\". We can make the task more efficient by splitting the data file such that each token contains the information about 1 job posting. **Splitting the text file into a list of job postings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30676\n"
     ]
    }
   ],
   "source": [
    "job_listings = re.split(r'-{30}\\n',file_contents)\n",
    "job_listings.pop() # remove last empty element\n",
    "print(len(job_listings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **30,676 job postings** in the data file.  \n",
    "Let us examine the patterns of a job posting more closely before we begin the extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ID: 39618\\ndeadline: 31 August 2007\\n_LOCS: Yerevan, Armenia\\nABOUT:\\n Square One Restaurants is a chain of restaurants.\\njob_desc: The Customer Service Representative will be responsible\\nfor the communication and clients' support by phone and in the office.\\nqualifications:\\n - University degree in Computer Sciences or a related field;\\n- 3-5 years of work experience in database design, development and\\noptimization technology;\\n- Excellent knowledge of OOP, T-SQL, PL-SQL, C#, ASP.net;\\n- Good knowledge of Armenian and Russian languages, knowledge of\\ntechnical English language;\\n- Problem-solving and decision-making skills;\\n- Good time management and organizational skills;\\n- Knowledge of accounting is a plus.\\nREMUNERATION/\\nSALARY: Competitive, based on work experience and\\neducational background.\\ntitle: Assistant to Executive Director\\nPROCEDURE: All interested and qualified candidates are\\nwelcome to send their CV to: info@... . Please indicate the\\nposition title in the subject field of your message. Only shortlisted\\ncandidates will be interviewed.\\nPlease clearly mention in your application letter that you learned of\\nthis job opportunity through Career Center and mention the URL of its\\nwebsite - www.careercenter.am, Thanks.\\nDATE_START: 21 December 2006\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_listings[25]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "The job posting has several sections with a section key followed by a colon. For example - 'ID:','_LOCS:','job_desc:',etc. The section keys are not consistent throughout the data file. The section keys are also not in the same order for every job posting. The list of sections are as follows:\n",
    "* ID\n",
    "* title\n",
    "* location\n",
    "* job_descriptions\n",
    "* job_responsibilities\n",
    "* required_qualifications\n",
    "* salary\n",
    "* application_procedure\n",
    "* start_date\n",
    "* application_deadline\n",
    "* about_company\n",
    "\n",
    "We need to write a regular expression for each section key. **The general approach is to observe the different keys for a section and write a regular expression to match all possible keys for the section.**  \n",
    "\n",
    "\n",
    "**Note that for the regex of the section keys, the search will only be at the start of a new line** because the keys are present only at beginning of a new line.  \n",
    "\n",
    "Writing regular expressions for each section key:   \n",
    "I. Regex for **ID**:<br>\n",
    "The listing must start with \"ID: \" followed by atleast a digit and end with a newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ID: '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ID Regex Pattern\n",
    "pattern_ID = re.compile(\"^(ID: )\\d+\\n\")\n",
    "m = pattern_ID.search(job_listings[436])\n",
    "m.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. Regex for **Title**:  \n",
    "  \n",
    "The possible keys are TITLES, TITLE, JOB TITLE, title, _TTL, JOB_T. The approach taken to write the regex is:  \n",
    "1. Write a regex to accomodate the first 5 keys with job and title: \"JOB \" can be optional at the beginning, followed by \"TITLE or title\" and \"S\" can be optional at the end\n",
    "2. Modify the expression to accomodate the key \"_TTL\" by making Underscore optional at the beginning and the \"TITLE\" part of the expression can have \"TTL\" as well; that part of the expression would now be \"TITLE | title | TTL\"\n",
    "3. Modify the expression to accomodate the key \"JOB_T\" by splitting \"JOB \" to \"JOB\" and \" \" and adding \"_T\" as optional after \"JOB\"  \n",
    "\n",
    "Finally, the regex here is to begin with underscore as optional, followed by \"JOB\" as optional, followed by \"_T\" as optional, space as optional, followed by \"TITLE | title | TTL\" as optional and finally \"S\" as optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'JOB TITLE: '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#title [TITLES,TITLE,JOB TITLE,title,_TTL,JOB_T]\n",
    "#1. pattern_title = re.compile(\"^(?:JOB )?(?:TITLE|title)[Ss]?: (.*?)\\n\",re.MULTILINE)\n",
    "#2. pattern_title = re.compile(\"^(?:JOB )?(?:TITLE|title|_TTL)[Ss]?: (.*?)\\n\",re.MULTILINE)\n",
    "#3. pattern_title = re.compile(\"^_?(?:JOB)?(?:_T)? ?(?:TITLE|title|TTL)?[Ss]?: (.*?)\\n\",re.MULTILINE)\n",
    "pattern_title = re.compile(\"^(_?(?:JOB)?(?:_T)? ?(?:TITLE|title|TTL)?S?: ).*?\\n\",re.M)\n",
    "m = pattern_title.search(job_listings[436])\n",
    "m.group(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III. Regex for **Location**:\n",
    "\n",
    "The possible keys are LOCATION, LOCATIONS, _LOCS, JOB_LOC.  \n",
    "\n",
    "The regex approach was quite straightforward, and the final expression was found within one iteration. The regex here is to begin with \"JOB\" as optional, followed by \"_\" as optional, followed by \"LOC\" (mandatory), followed by \"ATION\" as optional and finally \"S\" as optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_LOCS: '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Location:[LOCATION,LOCATIONS,_LOCS,JOB_LOC]\n",
    "#pattern_location = re.compile(\"^_?LOC(?:ATION)?S?: (.*?)\\n,\",re.MULTILINE)\n",
    "pattern_location = re.compile(\"^((?:JOB)?_?LOC(?:ATION)?S?: ).*?\\n\",re.M)\n",
    "m = pattern_location.search(job_listings[2])\n",
    "m.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IV. Regex for **Job Descriptions**: <br> \n",
    "\n",
    "The possible keys are JOB_DESC, JOB DESCRIPTION, job_desc, _description, DESCRIPTION,. \n",
    "\n",
    "The regex approach was quite straightforward, and the final expression was found within one iteration. The regex here is to begin with \"JOB | job\" as optional, followed by \"_\" or \" \" as optional, followed by \"DESC | desc\" (mandatory), followed by \"RIPTION | ription\" as optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_description: '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Job_Description[JOB_DESC, JOB DESCRIPTION,job_desc,_description,DESCRIPTION,]\n",
    "pattern_job_desc = re.compile(\"^((?:JOB|job)?[_ ]?(?:DESC|desc)(?:RIPTION|ription)?:\\s).*?\\n\",re.M)\n",
    "m = pattern_job_desc.search(job_listings[19])\n",
    "m.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V. Regex for **Job Responsibilities**:  \n",
    "    \n",
    "The possible keys are JOB_RESPS, RESPONSIBILITY, RESP, JOB RESPONSIBILITIES, responsibilities. The approach taken to write the regex is:  \n",
    "1. Write a regex to accomodate the first 3 keys: \"JOB_\" can be optional at the beginning, followed by \"RESP\" (mandatory), and \"ONSIBILITY | S\" can be optional at the end\n",
    "2. Modify the expression to accomodate the key \"JOB RESPONSIBILITIES\" by splitting 'JOB_\" to \"JOB\" and \"_\" optional at the beginning and the \"ONSIBILITY | S\" part of the expression can be split into \"ONSIBILIT\" and \"Y | IES\"\n",
    "3. Modify the expression to accomodate the key \"responsibilties\" by having either lower or upper case wherever ressponsibilties is present in the expression\n",
    "\n",
    "Finally, the regex here is to begin with \"JOB | job\" as optional, followed by \"_\" or \" \" as optional, \"RESP | resp\" (mandatory), followed by \"ONSIBILIT | onsibilit\" as optional and finally \"Y | IES | ies | y | S \" as optional. Note I am adding \"JOB | job\" at the beginning in case I missed a key with lower case \"job\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RESPONSIBILITY:\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[JOB RESPONSIBILITIES,JOB_RESPS,RESPONSIBILITY,RESP,responsibilities,]\n",
    "pattern_job_resp = re.compile(\"^((?:JOB|job)?[_ ]?(?:RESP|resp)(?:ONSIBILIT|onsibilit)?(?:Y|IES|ies|y|S)?:\\s).*?\\n\",re.M)\n",
    "m = pattern_job_resp.search(job_listings[498])\n",
    "m.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VI. Regex for **required qualifications**:\n",
    "\n",
    "The possible keys are qualifications, QUALIFICATION, REQUIRED QUALIFICATIONS, QUALIFS, REQ_QUAL.\n",
    "\n",
    "The approach taken to form the regex is:\n",
    "- Create a regex for the first 3 keys: start with \"REQUIRED\" as optional, then space as optional, followed by \"QUALIFICATION | qualification (mandatory)\" and \"S | s\" as optional\n",
    "- Modify the regex to accomodate the last 2 keys by splitting \"REQUIRED\" into \"REQ\" as optional, followed by underscore as optional and \"UIRED\" as optional; splitting \"QUALIFICATION | qualification\" into \"QUAL | qual\" (mandatory), followed by \"IF | IFICATION | ification\" as optional and finally \"S | s as\" optional.\n",
    "\n",
    "Finally, the regex here is to begin with \"REQ\" as optional, followed by underscore as optional and \"UIRED\" as optional, space as optional, \"QUAL | qual\" (mandatory), followed by \"IF | IFICATION | ification\" as optional and finally \"S | s\" as optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qualifications:\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[REQ_QUALS,qualifications,QUALIFICATION,REQUIRED QUALIFICATIONS,QUALIFS]\n",
    "pattern_qual = re.compile(\"^((?:REQ)?[_]?(?:UIRED)? ?(?:QUAL|qual)?(?:IF|IFICATION|ification)?[Ss]?:\\s).*?\\n\",re.M)\n",
    "m = pattern_qual.search(job_listings[498])\n",
    "m.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VII. Regex for **Job Salary**: \n",
    "\n",
    "The possible keys are JOB_SAL, SALARY, salary, remuneration, REMUNERATION.  \n",
    "I wasn't able to come up with a regex similar to other sections' regex because this section has very varied keys. The regex here is to have the line start with any of the above section keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'JOB_SAL: '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Salary [JOB_SAL,SALARY,salary,remuneration,REMUNERATION]\n",
    "pattern_sal = re.compile(\"^((?:JOB_SAL|SALARY|salary|REMUNERATION|remuneration)?:\\s).*?\\n\",re.M)\n",
    "m = pattern_sal.search(job_listings[0])\n",
    "m.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIII. Regex for **Application procedure**: \n",
    "\n",
    "The possible keys are PROCEDURES, procedures, JOB_PROC, JOB_PROCS, PROCEDURE, procedure.  \n",
    "\n",
    "This regex is quite straightforward. The regex here is to begin with \"JOB\" as optional, then underscore as optional, followed by \"PROC | proc\" (mandatory), \"EDURE | edure\" as optional, and finally \"S | s\" as optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROCEDURES: '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[PROCEDURES,procedures,JOB_PROC,JOB_PROCS,PROCEDURE, procedure]\n",
    "pattern_proc = re.compile(\"^((?:JOB)?[_]?(?:PROC|proc)(?:EDURE|edure)?[Ss]?:\\s).*?\\n\",re.M)\n",
    "m = pattern_proc.search(job_listings[0])\n",
    "m.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IX. Regex for **Start Date**: \n",
    "\n",
    "The possible keys are start_date, DATE_START, START DATE, START_DA, DATES.  \n",
    "\n",
    "Creating this regex is also straighforward. The regex here is to begin with \"START | start\" as optional, then \"underscore | space\" as optional, followed by \"DA | da\" (mandatory), \"TE | te\" as optional, \"_START\" as optional and finally \"S as optional\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DATES: '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[start_date,DATE_START,START DATE,START_DA,DATES]\n",
    "pattern_start_date = re.compile(\"^((?:START|start)?[_ ]?(?:DA|da)(?:TE|te)?(?:_START)?[S]?:\\s).*?\\n\",re.M)\n",
    "m = pattern_start_date.search(job_listings[1300])\n",
    "m.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X. Regex for **Application Deadline**: \n",
    "\n",
    "The possible keys are DEADLINES, DEAD_LINE, deadline, APPLICATION_DEADL, Application_DL.  \n",
    "The approach taken to form the regex is:\n",
    "- Create a regex for the first 4 keys: start with \"APPLICATION\" as optional, then underscore as optional, followed by \"DEAD | dead\" (mandatory), underscore as optional, \"LINE | line\" as optional and finally \"S | s\" as optional\n",
    "- Modify the regex to accomodate the last keys by adding \"| DL\" to the mandatory portion; the mandatory portion would now be \"DEAD | dead | DL\".  Split \"LINE | line\" as optional to \"L | l\" as optional, \"INE | ine\" as optional \n",
    "\n",
    "Finally, the regex here is to start with 'APPLICATION' as optional, then underscore as optional, followed by \"DEAD | dead | DL\" (mandatory), underscore as optional, \"L | l\" as optional, \"INE | ine\" as optional and finally \"S | s\" as optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'APPLICATION_DEADL: '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[DEADLINES,DEAD_LINE,deadline,APPLICATION_DEADL,Application_DL]\n",
    "pattern_deadline = re.compile(\"^((?:APPLICATION)?[_]?(?:DEAD|dead|DL)[_]?[Ll]?(?:INE|ine)?[Ss]?:\\s).*?\\n\",re.M)\n",
    "m = pattern_deadline.search(job_listings[1173])\n",
    "m.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XI. Regex for **information about the company**:  \n",
    "  \n",
    "The possible keys are about_company, ABOUT COMPANY, ABOUT, COMPANYS_INFO, _info.  \n",
    "The approach taken to form the regex is:\n",
    "- Create a regex for the first 3 keys: start with \"ABOUT | about\", then \"underscore | space\" as optional, followed by \"COMPANY | company\" as optional\n",
    "- Modify the regex to accomodate the last 2 keys by appending \"S\" as optional,underscore as optional and \"INFO | info\" as optional. Change the \"ABOUT | about\" to optional.  \n",
    "\n",
    "Finally, the regex here is to start with \"ABOUT | about\" as optional, then \"underscore | space\" as optional, followed by \"COMPANY | company\" as optional, \"S\" as optional, underscore as optional and finally \"INFO | info\" as optional. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABOUT COMPANY:\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[about_company,ABOUT COMPANY,ABOUT,COMPANYS_INFO,_info,]\n",
    "pattern_info = re.compile(\"^((?:ABOUT|about)?[_ ]?(?:COMPANY|company)?S?[_]?(?:INFO|info)?:\\s).*?\\n\",re.M)\n",
    "m = pattern_info.search(job_listings[2])\n",
    "m.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing all the compiled regular expressions in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regex_dict = {\n",
    "    'ID' : pattern_ID,\n",
    "    'title' : pattern_title,\n",
    "    'location' : pattern_location,\n",
    "    'job_descriptions' : pattern_job_desc,\n",
    "    'job_responsibilities' : pattern_job_resp,\n",
    "    'required_qualifications' : pattern_qual,\n",
    "    'salary' : pattern_sal,\n",
    "    'application_procedure' : pattern_proc,\n",
    "    'start_date' : pattern_start_date,\n",
    "    'application_deadline' : pattern_deadline,\n",
    "    'about_company' : pattern_info\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While writing the regex for section keys, I noticed that there is some **garbage** line like OPEN TO/, RENUMERATION/, START DATE/, etc. before a few keys. Below is a **regular expression** to catch that so that we can **exlude** that from the extracts:  \n",
    "The regex here would be to start with a newline, followed by a capital alphabet, followed by text and then \" / \" at the end. There are few cases with multiple newlines before this garbage like \"\\n\\n\\nOPEN TO/\", we need to modify the expression to accomodate multiple newlines by doing \\n{1,4} and use the re.S flag to not end at a newline. Final expression is in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extract_garbage_rx = re.compile(\"(.*?)\\n{1,4}[A-Z](?:[\\w ])+?/$\",re.S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extracting the data\n",
    "\n",
    "All the regular expressions are now written, it is time to begin extracting the data. I am using a function to parse each job listing, extract the relevant data for each section and **store the data in a dictionary** with the key as the section key. For example, for joblisting 1, the dictionary would be 'ID' :  '69290', 'title' : 'IT Reporting System Administration Senior Specialist',... and so on.  \n",
    "\n",
    "The **extraction methodology** for a listing is:  \n",
    "1. Match the section keys. If there is no match for a particular section, put \"N/A\" to indicate missing section. Some listings do not have some sections. For example, the first listing with ID '69290' doesn't contain the 'job description' section \n",
    "2. Retrieve the exact sections keys,for example - '_TTL' for 'title'\n",
    "3. Retrieve the start indices of all the section keys in a listing\n",
    "3. Sort the start indices of the section keys to get the order of the sections in the listing\n",
    "4. Use string splicing to extract the relevant data for that section. For splicing, we need the start and stop index for the extract; the start index will be the start index of the section key + length(exact section key) and the stop index will be start index of the next section - 1\n",
    "5. Use the garbage regex defined above to check if you have garbage like OPEN TO/, START DATE/, etc. in the extract. If garbage is present, remove the garbage fromt he extract\n",
    "6. Store the clean extract for the section with its section key in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def listing_parser(listing):\n",
    "    sec_index = {}\n",
    "    sec_wording = {}\n",
    "    sec_data = {}\n",
    "    sec_missing = []\n",
    "    for key, pattern in regex_dict.items():\n",
    "        # Matching section keys\n",
    "        match = pattern.search(listing)\n",
    "        if match:\n",
    "            # Retrieving the exact key and start index of the key\n",
    "            sec_index[match.start(1)] = key\n",
    "            sec_wording[match.start(1)] = match.group(1)\n",
    "        else:\n",
    "            sec_missing.append(key)\n",
    "    # Ordering the start indices of the key\n",
    "    ordered_sec_index = []\n",
    "    for key in sec_index.keys():\n",
    "        ordered_sec_index.append(key)\n",
    "    ordered_sec_index.sort()\n",
    "    # Extracting the relevant data using string splicing\n",
    "    for i in range(len(ordered_sec_index)):\n",
    "        extract_start_index = ordered_sec_index[i] + len(sec_wording[ordered_sec_index[i]])\n",
    "        if i == len(ordered_sec_index) - 1: # Special case for the last section key\n",
    "            extract_stop_index = len(listing) - 1\n",
    "        else:\n",
    "            extract_stop_index = ordered_sec_index[i + 1] - 1\n",
    "        # Splicing to get the extarct\n",
    "        extract = listing[extract_start_index:extract_stop_index]\n",
    "        \n",
    "        #Removing garbage things like OPEN TO/, RENUMERATION/, START DATE/, etc. at the end of a few extracts\n",
    "        if len(extract) > 0 and extract[-1] == \"/\":\n",
    "            match = extract_garbage_rx.search(extract)\n",
    "            if match:\n",
    "                extract = match.group(1)\n",
    "        \n",
    "        sec_data[sec_index[ordered_sec_index[i]]] = extract\n",
    "    # Adding N/A for missing sections in a listing\n",
    "    for section in sec_missing:\n",
    "        sec_data[section] = 'N/A'\n",
    "    return sec_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking the function** we defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': '69290',\n",
       " 'job_responsibilities': ' - Develop online/ mobile games working with the team very closely (being\\na team player, not a solo);\\n- Work with Designers and Illustrators on artwork and design\\nimplementation into the games;\\n- Define specifications of game features together with Product Managers;\\n- Develop and architect different types of frameworks and toolsets;\\n- Constantly learn and grow your skills.',\n",
       " 'application_deadline': '25 January 2014',\n",
       " 'location': 'Yerevan, Armenia',\n",
       " 'about_company': ' Arplan LLC is an architectural consulting company\\nworking on international projects.',\n",
       " 'required_qualifications': ' - Minimum of Masters degree in education, training and/or training\\nmethodology; \\n- Minimum of ten years work experience as a trainer, curriculum\\ndeveloper, or workforce development administrator;\\n- Fluency in English language;\\n- Fluency in Armenian or Russian is preferred.',\n",
       " 'salary': 'Competitive, based on previous experience and\\nprofessional skills.',\n",
       " 'title': 'IT Reporting System Administration Senior Specialist',\n",
       " 'application_procedure': 'Applications should be sent to:hrselection@....\\nPlease include your CV in the body of the message, not as an attachment.\\nPlease clearly mention in your application letter that you learned of\\nthis job opportunity through Career Center and mention the URL of its\\nwebsite - www.careercenter.am, Thanks.',\n",
       " 'start_date': '04 July 2012',\n",
       " 'job_descriptions': 'N/A'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listing_parser(job_listings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the parser function for all the listings**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_extracted_data = []\n",
    "for listing in job_listings:\n",
    "    each_extract = listing_parser(listing)\n",
    "    full_extracted_data.append(each_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(full_extracted_data) # list of dictionaries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For some sections, the information is in the form of a list**; for example, the responsibilites section has a list of resposiblities in its extracted information. Therefore, it is useful to **convert the extracted information for these sections into a list.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub_sections_dict = {\n",
    "    'job_responsibilities' : 'responsibility',\n",
    "    'required_qualifications' : 'qualification',\n",
    "    'job_descriptions' : 'description'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For splitting the extracted information into lists, we need to identfiy the **splitting pattern**. After observing the data, I came up with the following 2 possible patterns:  \n",
    "**Pattern I**. Section Key:   \n",
    "1. Text:   \n",
    "    - Sub-bullet text     \n",
    "    - Sub-bullet text  \n",
    "2. Text:  \n",
    "     - Sub-bullet text  \n",
    "     - ...\n",
    "  \n",
    "For example,  \n",
    "RESPONSIBILITY:  \n",
    "   1. Publicity/Program activities:  \n",
    "        - Organize country-wide Internet conferences/thematic web chats  \n",
    "   2. Administration:  \n",
    "        - Process weekly and monthly site reports and produce regular feedback to the staff  \n",
    "\n",
    "**Pattern II**. Section Key:  \n",
    "      - Sub-bullet text  \n",
    "      - Sub-bullet text  \n",
    "       ....  \n",
    "For example,  \n",
    "   RESPONSIBILITY:  \n",
    "     - Work with the product team to define functional requirements;  \n",
    "    - Produce customer and other third party facing product documentation.  \n",
    "\n",
    "Below, I am splitting the extracted information for an example for each of the patterns:  \n",
    "**Pattern I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Publicity/Program activities:\\n- Organize country-wide Internet conferences/thematic web chats \\n- Draft and/or supervise various sub-projects (Volunteer and Intern\\nPrograms, interaction with Peace Corps Volunteers and local NGOs, user\\nsurveys, etc.)\\n- Prepare weekly program news for submission to regional management and\\nUS Department of State\\n- Prepare weekly news briefs (in English and Armenian) for the local\\nprogram website\\n- Support TC and CC with distance learning project as needed \\n',\n",
       " ' Administration:\\n- Process weekly and monthly site reports and produce regular feedback\\nto the staff\\n- Assist IATP Country Coordinator in writing country reports\\n- Distribute various administrative/program announcements to field\\nstaff\\n- Answer various field requests or forward them to the appropriate\\nmanager\\nOther duties as assigned.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pattern I\n",
    "extract_split = re.split(\" ?\\w\\)(.*?:)\\n\",full_extracted_data[4]['job_responsibilities'])\n",
    "sub_section_data = []\n",
    "if len(extract_split) > 1:\n",
    "    for i in range(len(extract_split)):\n",
    "        if extract_split[i] == '' or extract_split[i][0] == '-':\n",
    "            continue\n",
    "        elif extract_split[i][-1] == ':':\n",
    "            join = extract_split[i] + '\\n' + extract_split[i + 1]\n",
    "            sub_section_data.append(join)\n",
    "        else:\n",
    "            sub_section_data.append(extract_split[i])\n",
    "sub_section_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The splitting worked successfully for pattern I, splitting for an example of pattern II:  \n",
    "**Pattern II**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' - Develop online/ mobile games working with the team very closely (being\\na team player, not a solo)',\n",
       " ' Work with Designers and Illustrators on artwork and design\\nimplementation into the games',\n",
       " ' Define specifications of game features together with Product Managers',\n",
       " ' Develop and architect different types of frameworks and toolsets',\n",
       " ' Constantly learn and grow your skills.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_split_2_rx = re.split(\";?\\n-\",full_extracted_data[0]['job_responsibilities'])\n",
    "sub_split_2_rx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the **splitting for all the listings**:\n",
    "The steps for this are - \n",
    "- Check if the section key is of one of the sections for which splitting is required. Implement the remaining steps only for the sections with possible sub-sections\n",
    "- If the information follows pattern 1, split based on pattern 1; else split based on pattern 2. If the information doesn't fall under one of the 2 patterns, have one element in the list (sub-section) with the entire extracted information for that section\n",
    "- Overwrite the information with the updated information after splitting to the full_extracted_data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adding sub-sections for responsibilities, qualifications and job description\n",
    "# Splitting extracted information for these sections\n",
    "for listing in full_extracted_data:\n",
    "    for key, extract in listing.items():\n",
    "        if key in sub_sections_dict.keys():\n",
    "            sub_sec_data_dict = {}\n",
    "            sub_section_data = []\n",
    "            if key == 'job_descriptions':\n",
    "                s = extract.replace('\\n',' ')\n",
    "                sub_section_data.append(s)\n",
    "            else:\n",
    "                # Pattern 1\n",
    "                extract_split = re.split(\" ?\\w\\)(.*?:)\\n\",str(extract))\n",
    "                if len(extract_split) > 1:\n",
    "                    for i in range(len(extract_split)):\n",
    "                        if extract_split[i] == '' or extract_split[i][0] == '-':\n",
    "                            continue\n",
    "                        elif extract_split[i][-1] == ':':\n",
    "                            join = extract_split[i] + '\\n' + extract_split[i + 1]\n",
    "                            join = join.replace('\\n','') # remove newlines\n",
    "                            sub_section_data.append(join)\n",
    "                        else:\n",
    "                            s = extract_split[i].replace('\\n','') # remove newlines\n",
    "                            sub_section_data.append(s)\n",
    "                else: # Pattern 2\n",
    "                    extract_split_2 = re.split(\";?\\n-\",str(extract))\n",
    "                    if len(extract_split_2) > 1:\n",
    "                        for i in range(len(extract_split_2)):\n",
    "                            if i == 0:\n",
    "                                if extract_split_2[0][1] == '-':\n",
    "                                    first_element_extract = extract_split_2[0][2:]\n",
    "                                    first_element_extract = first_element_extract.replace('\\n',' ') # remove newlines \n",
    "                                    sub_section_data.append(first_element_extract)\n",
    "                                else:\n",
    "                                    s = extract_split_2[i].replace('\\n',' ') # remove newlines\n",
    "                                    sub_section_data.append(s)\n",
    "                            else:\n",
    "                                s = extract_split_2[i].replace('\\n',' ') # remove newlines\n",
    "                                sub_section_data.append(s)\n",
    "                    else: # None of the patterns\n",
    "                        s = extract.replace('\\n',' ')\n",
    "                        sub_section_data.append(s)\n",
    "            # Overwriting the information to the full_extracted_data dictionary\n",
    "            sub_sec_data_dict[sub_sections_dict[key]] = sub_section_data\n",
    "            listing[key] = sub_sec_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing newlines from the extracted information** for all the sections. Note that for the sections with sub-sections, newlines have already been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for listing in full_extracted_data:\n",
    "    for key, extract in listing.items():\n",
    "        if key not in sub_sections_dict.keys():\n",
    "            extract = extract.replace('\\n',' ') # remove newlines\n",
    "            listing[key] = extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ID': '69290', 'job_responsibilities': {'responsibility': [' Develop online/ mobile games working with the team very closely (being a team player, not a solo)', ' Work with Designers and Illustrators on artwork and design implementation into the games', ' Define specifications of game features together with Product Managers', ' Develop and architect different types of frameworks and toolsets', ' Constantly learn and grow your skills.']}, 'application_deadline': '25 January 2014', 'location': 'Yerevan, Armenia', 'about_company': ' Arplan LLC is an architectural consulting company working on international projects.', 'required_qualifications': {'qualification': [' Minimum of Masters degree in education, training and/or training methodology; ', ' Minimum of ten years work experience as a trainer, curriculum developer, or workforce development administrator', ' Fluency in English language', ' Fluency in Armenian or Russian is preferred.']}, 'salary': 'Competitive, based on previous experience and professional skills.', 'title': 'IT Reporting System Administration Senior Specialist', 'application_procedure': 'Applications should be sent to:hrselection@.... Please include your CV in the body of the message, not as an attachment. Please clearly mention in your application letter that you learned of this job opportunity through Career Center and mention the URL of its website - www.careercenter.am, Thanks.', 'start_date': '04 July 2012', 'job_descriptions': {'description': ['N/A']}}\n"
     ]
    }
   ],
   "source": [
    "print(full_extracted_data[0]) # quality check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Writing the extracted information to a JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extracted data was transformed into json format using the **json dump** method in the json library. A dictionary is created using the full_extracted_data dictionary to get the output in the desired format; the 'listings' and 'listing' key are added. The json dictionary is now a dictionary of dicitionaries. The json dictionary is passed to the json dump method to write the extracted information to a json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Making the JSON dictionary\n",
    "json_dict = {'listings' : {'listing' : full_extracted_data}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('job_postings.json', 'w') as output_file:  \n",
    "    json.dump(json_dict, output_file,indent=4)\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Writing the extracted information to an XML file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extracted information has beem transformed to XML format using string/list methods in Python.  \n",
    "  \n",
    "Define the **initial line and the root tag** in the XML tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n', '<listings>\\n']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_output = ['<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n'] #initial line declaring the text encoding format\n",
    "xml_output.append('<listings>\\n') #root start tag\n",
    "xml_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replacing '<' and '&'** in the extracted information with **'&lt' and '&amp'** respectively:  \n",
    "\n",
    "This is done because the XML parser will interpret the special meaning of these characters. For example, the parser will interpret \"<\" as the start of a new element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for listing in full_extracted_data:\n",
    "    for key, extract in listing.items():\n",
    "        if key not in sub_sections_dict.keys():\n",
    "            extract = extract.replace('<','&lt;')\n",
    "            extract = extract.replace('&','&amp;')\n",
    "            listing[key] = extract\n",
    "        else:\n",
    "            for subsection, content in extract.items():\n",
    "                xml_content = []\n",
    "                for item in content:\n",
    "                    item = item.replace('&','&amp;')\n",
    "                    item = item.replace('<','&lt;')\n",
    "                    xml_content.append(item)\n",
    "            extract[subsection] = xml_content "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Defining a function to convert the passed extract content of section/sub-section to XML format by adding the passed number of tabs, start tag, extract content and the end tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\t\\t<title>IT Reporting System Administration Senior Specialist</title>\\n']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def writing_extract(key,extract,no_of_tabs):\n",
    "    ext_output = []\n",
    "    ext_output.append('\\t'*no_of_tabs + '<' + key + '>' + extract + '</' + key + '>\\n')\n",
    "    return ext_output\n",
    "\n",
    "writing_extract('title', full_extracted_data[0]['title'],2) # example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a **function to transform the extract of the listing to XML format**. It iterates through the extract dictionary, and uses the 'writing_extract' function defined above to transform the extract for the section/sub-section to XML Format. The function returns the output string with the extracted listing in XML style. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def xml_transformation(listing_extract):\n",
    "    output = []\n",
    "    # adding elements corresponding to the sections of the listing \n",
    "    for key, extract in listing_extract.items():\n",
    "        if key == 'ID':\n",
    "            output.append(list('\\t<listing id=\"' + extract + '\">\\n'))\n",
    "        elif key in sub_sections_dict.keys():\n",
    "            output.append(list('\\t\\t<'+ key + '>\\n'))\n",
    "            # 3 tabs for sub-sections\n",
    "            for subsection, content in extract.items():\n",
    "                for item in content:\n",
    "                    output.append(writing_extract(subsection,item,3))\n",
    "            output.append(list('\\t\\t</'+ key + '>\\n'))\n",
    "        else: # 2 tabs for sections\n",
    "            output.append(writing_extract(key,extract,2))\n",
    "    output.append(list('\\t</listing>\\n'))    \n",
    "    output_flat = list(sum(output, []))\n",
    "    output_str = ''.join(output_flat)\n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the function defined above for all listings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for listing_extract in full_extracted_data:\n",
    "    xml_output.append(xml_transformation(listing_extract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n',\n",
       " '<listings>\\n',\n",
       " '\\t<listing id=\"69290\">\\n\\t\\t<job_responsibilities>\\n\\t\\t\\t<responsibility> Develop online/ mobile games working with the team very closely (being a team player, not a solo)</responsibility>\\n\\t\\t\\t<responsibility> Work with Designers and Illustrators on artwork and design implementation into the games</responsibility>\\n\\t\\t\\t<responsibility> Define specifications of game features together with Product Managers</responsibility>\\n\\t\\t\\t<responsibility> Develop and architect different types of frameworks and toolsets</responsibility>\\n\\t\\t\\t<responsibility> Constantly learn and grow your skills.</responsibility>\\n\\t\\t</job_responsibilities>\\n\\t\\t<application_deadline>25 January 2014</application_deadline>\\n\\t\\t<location>Yerevan, Armenia</location>\\n\\t\\t<about_company> Arplan LLC is an architectural consulting company working on international projects.</about_company>\\n\\t\\t<required_qualifications>\\n\\t\\t\\t<qualification> Minimum of Masters degree in education, training and/or training methodology; </qualification>\\n\\t\\t\\t<qualification> Minimum of ten years work experience as a trainer, curriculum developer, or workforce development administrator</qualification>\\n\\t\\t\\t<qualification> Fluency in English language</qualification>\\n\\t\\t\\t<qualification> Fluency in Armenian or Russian is preferred.</qualification>\\n\\t\\t</required_qualifications>\\n\\t\\t<salary>Competitive, based on previous experience and professional skills.</salary>\\n\\t\\t<title>IT Reporting System Administration Senior Specialist</title>\\n\\t\\t<application_procedure>Applications should be sent to:hrselection@.... Please include your CV in the body of the message, not as an attachment. Please clearly mention in your application letter that you learned of this job opportunity through Career Center and mention the URL of its website - www.careercenter.am, Thanks.</application_procedure>\\n\\t\\t<start_date>04 July 2012</start_date>\\n\\t\\t<job_descriptions>\\n\\t\\t\\t<description>N/A</description>\\n\\t\\t</job_descriptions>\\n\\t</listing>\\n']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_output[:3] # quality check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Closing the root tag** 'Listings'\n",
    "- Converting the list with xml outputs of each listing to a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xml_output.append('</listings>\\n')\n",
    "xml_output_str = ''.join(xml_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Writing** the xml output string to a **file**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('job_postings.xml','w') as output_file:\n",
    "    output_file.write(xml_output_str)\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "The task demonstrated the process of extracting data from large unstructured text files and transforming the data to XML and JSON format. The extraction was done by identifying the patterns of the section keys and writing corresponding efficient regular expressions. The data transformation to XML format was done manually, helping us get a very good grasp of the XML structure. The XML transformation also required us to identify the patterns in the data to split certain sections like responsibilities to sub-sections.  \n",
    "\n",
    "The other learnings from this task, not necessarily specific to the process of data wrangling, were:\n",
    "- The general approach to extracting data is to identify patterns in the data and then use appropriate methods like regex, splitting, etc. to complete the data extraction. \n",
    "-  When parsing a huge file, write as efficient code as possible. Consider the time and space complexity of the code to extract the information from a huge data file. In this task, the following key things were done to improve the efficieny of the code so that the run time of the code was less than a minute: \n",
    "    1. Compiling all regex patterns and saving the regular expression object for reuse because the same regular expressions were used several times for all the job listings\n",
    "    2. Try to avoid string concatenation for large strings as much as possible, since the complexity for string concatenation is O(n). Use str.join() or use a list to leverage the O(1) list.append() method\n",
    "- Parsing a huge file can seem very daunting at the beginning. The key is to break down the process to several steps; solve the smaller steps and combine the solutions of the smaller steps to get the desired results / output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. *XML Syntax*. Retrieved from https://www.w3schools.com/xml/xml_syntax.asp\n",
    "2. The Python Standard Library. *Regular expression operations documentation*: `re.search`. Retrieved from https://docs.python.org/3/library/re.html\n",
    "3. The Python Standard Library. *JSON Encoder and Decoder*: `json.dump`. Retrieved from\n",
    " https://docs.python.org/3/library/json.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
